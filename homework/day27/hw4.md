# homework4：Seq2seq翻译项目的原理理解，绘制自己理解的seq2seq原理图  

## 1 Seq2seq翻译项目的原理理解

### 1.1原始Seq2Seq模型

#### 1.1.1 基本结构

原始Seq2Seq模型由两个主要部分组成：编码器（Encoder）和解码器（Decoder），通常基于RNN（如LSTM或GRU）。

- 编码器：  
  - 输入是一个序列（如句子），编码器逐步处理每个时间步的输入，最终生成一个固定长度的上下文向量（Context Vector），用于表示整个输入序列的信息。
  - 公式表示：  
  $$h_t = f(h_{t-1}, x_t)$$  
  其中$h_t$是当前时间步的隐状态，$h_{t-1}$是上一时间步的隐状态，$x_t$是当前时间步的输入。$f$是编码器的RNN单元。
  - 编码器的最终隐藏状态$h_T$作为整个输入序列的上下文向量$c$，传递给解码器。
- 解码器：
  - 解码器根据上下文向量$c$逐步生成输出序列。
  - 公式表示：  
  $$y_t = g(y_{t-1}, s_t, c)$$  
  其中$y_t$是当前时间步的输出，$y_{t-1}$是上一时间步的输出，$s_t$是当前时间步的隐藏状态，$c$是整个输入序列的上下文向量。$g$是解码器的RNN单元。
  - 解码器的输出通过一个softmax层生成概率分布，用于预测下一个词。

#### 1.1.2 局限性

- 信息瓶颈：  
  - 编码器需要将整个输入序列压缩为一个固定长度的上下文向量，这会导致信息丢失，尤其是长序列。
- 长距离依赖：  
  - RNN在处理长序列时容易出现梯度消失或爆炸的问题，难以捕捉长距离依赖。

### 1.2 基于注意力机制的Seq2Seq模型

#### 1.2.1 基本思想

为了解决原始Seq2Seq模型的信息瓶颈和长距离依赖问题，注意力机制被引入。注意力机制允许解码器在生成每个输出时，动态地关注输入序列的不同部分，而不是依赖单一的上下文向量。

#### 1.2.2 注意力机制的原理

- 编码器：
  - 编码器依然使用RNN处理输入序列，但不再生成单一的上下文向量，而是保留所有时间步的隐藏状态。
- 注意力计算：
  - 在解码器的每个时间步$t$，计算一个注意力权重$a_t$，表示解码器当前时间步对编码器第$i$个隐藏状态的关注程度。
  - 公式表示：
  $$e_{ti} = \text{score}(s_{t-1}, h_i)$$
  $$a_{ti} = \frac{\text{exp}(e_{ti})}{\sum_{j=1}^{T} \text{exp}(e_{tj})}$$
  其中$s_{t-1}$是解码器上一时间步的隐藏状态，$h_i$是编码器的第$i$个隐藏状态，$score$是一个评分函数（如点积、加性注意力等）$T$是编码器的总时间步数。
  - 注意力权重$a_{ti}$用于加权编码器的隐藏状态，生成一个上下文向量$c_t$：
  $$c_t = \sum_{i=1}^{T} a_{ti} h_i$$
  - 解码器：
    - 解码器在生成每个输出时，不仅依赖上一个时间步的隐藏状态$s_{t-1}$和输出$y_{t-1}$，还依赖当前时间步的上下文向量$c_t$。
    - 公式表示：
    $$y_t = g(y_{t-1}, s_{t-1}, c_t)$$
    其中$g$是解码器的RNN单元。

#### 1.2.3 优点

- 动态上下文：
  - 每个时间步的上下文向量$c_t$是动态生成的，能够捕捉输入序列中与当前输出相关的部分。
- 长距离依赖：
  - 注意力机制直接建模输入和输出序列之间的关系，缓解了长距离依赖问题。
- 信息保留：
  - 编码器保留所有时间步的隐藏状态，而不是生成单一的上下文向量，能够保留输入序列中的丰富信息。
  
## 2 LSTM和GRU

### 2.1 LSTM

LSTM（Long Short-Term Memory）是一种特殊的RNN，它在内部引入了门（gate）结构，使得它能够学习长期依赖。

#### 2.1.1 LSTM网络的循环单元结构

![LSTM循环单元结构](../images/day27/LSTM网络的循环单元结构.png)

#### 2.1.2 LSTM公式

$$f_t = \sigma(W_f · [h_{t-1},x_t] + b_f)$$
$$i_t = \sigma(W_i · [h_{t-1},x_t] + b_i)$$
$$\tilde{C_t} = \tanh(W_C · [h_{t-1},x_t] + b_C)$$
$$C_t = f_t * C_{t-1} + i_t * \tilde{C_t}$$
$$o_t = \sigma(W_o · [h_{t-1},x_t] + b_o)$$
$$h_t = o_t * \tanh(C_t)$$

其中：

- $[h_{t-1},x_t]$是输入，$W_f,W_i,W_C,W_o$是权重矩阵，$b_f,b_i,b_C,b_o$是偏置向量。
- $\sigma$是sigmoid函数。
- $f_t,i_t,o_t$是门（gate）的输出，$C_t$是单元的输出，$h_t$是单元的隐藏状态。

### 2.2 GRU

GRU 即 Gated Recurrent Unit。前面说到为了克服 RNN 无法很好处理远距离依赖而提出了LSTM，而 GRU 则是 LSTM 的一个变体，当然 LSTM 还有有很多其他的变体。GRU 保持了LSTM的效果同时又使结构更加简单，所以它也非常流行
