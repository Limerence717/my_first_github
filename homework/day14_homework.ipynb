{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### homework1：完成DictVectorizer，CountVectorizer, TfidfVectorizer的练习，并理解其原理",
   "id": "bee3cde542b365cc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T09:45:48.877892Z",
     "start_time": "2025-01-09T09:45:48.116220Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DictVectorizer",
   "id": "dda03bf5dfd8dd1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:52:28.102590Z",
     "start_time": "2025-01-09T09:52:28.095372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dict_vectorizer_demo():\n",
    "    \"\"\"\n",
    "    字典数据抽取示例\n",
    "    将字典数据转换为特征矩阵，展示特征名称和反向转换结果\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 初始化 DictVectorizer 对象，设置 sparse=True 表示返回稀疏矩阵\n",
    "    dictv = DictVectorizer(sparse=True)\n",
    "    # 初始化 DictVectorizer 对象，设置 sparse=False 表示返回稠密矩阵\n",
    "    # dict1 = DictVectorizer(sparse=False)\n",
    "\n",
    "    # 定义字典数据，包含城市和温度信息\n",
    "    # fit_transform 的作用是同时完成 拟合（学习数据特性） 和 转换（将数据转换为模型可用的格式）。\n",
    "    data = dictv.fit_transform([{'city': '北京', 'temperature': 100},\n",
    "                                {'city': '上海', 'temperature': 60},\n",
    "                                {'city': '深圳', 'temperature': 30}])\n",
    "\n",
    "    # 打印转换后的特征矩阵（稀疏矩阵格式）\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 打印特征名称（即转换后的列名）\n",
    "    print(dictv.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 将特征矩阵反向转换为原始字典格式\n",
    "    print(dictv.inverse_transform(data))\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用函数，运行示例\n",
    "dict_vectorizer_demo()"
   ],
   "id": "f32835b27e9ec0a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 6 stored elements and shape (3, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t100.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t60.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t30.0\n",
      "--------------------------------------------------\n",
      "['city=上海' 'city=北京' 'city=深圳' 'temperature']\n",
      "--------------------------------------------------\n",
      "[{'city=北京': np.float64(1.0), 'temperature': np.float64(100.0)}, {'city=上海': np.float64(1.0), 'temperature': np.float64(60.0)}, {'city=深圳': np.float64(1.0), 'temperature': np.float64(30.0)}]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CountVectorizer",
   "id": "a09c9ab71970ae6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:54:24.740565Z",
     "start_time": "2025-01-09T09:54:24.729945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_vectorizer_demo():\n",
    "    \"\"\"\n",
    "    文本特征提取示例\n",
    "    使用 CountVectorizer 将文本数据转换为词频矩阵\n",
    "    对文本进行特征值化,单个汉字单个字母不统计，因为单个汉字字母没有意义\n",
    "    对汉字不能用空格来进行分割，所以需要使用 jieba 分词工具进行分词\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 初始化 CountVectorizer 对象\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    # 输入文本数据\n",
    "    data = cv.fit_transform([\"人生苦短，我 喜欢 python python\", \"人生漫长，不用 python\"])\n",
    "\n",
    "    # 打印特征名称（即词汇表中的单词）\n",
    "    print(\"特征名称（词汇表）：\")\n",
    "    print(cv.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 打印稀疏矩阵（只记录非零值的位置）\n",
    "    print(\"稀疏矩阵：\")\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 将稀疏矩阵转换为稠密矩阵（二维数组），并打印\n",
    "    print(\"稠密矩阵（词频矩阵）：\")\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用函数，运行示例\n",
    "count_vectorizer_demo()"
   ],
   "id": "56e4e8e43dc00c74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征名称（词汇表）：\n",
      "['python' '不用' '人生漫长' '人生苦短' '喜欢']\n",
      "--------------------------------------------------\n",
      "稀疏矩阵：\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 6 stored elements and shape (2, 5)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "--------------------------------------------------\n",
      "稠密矩阵（词频矩阵）：\n",
      "[[2 0 0 1 1]\n",
      " [1 1 1 0 0]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "掌握如何对中文进行分词",
   "id": "2414eeefe6767a54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:58:41.280245Z",
     "start_time": "2025-01-09T09:58:40.930987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cutword():\n",
    "    \"\"\"\n",
    "    通过 jieba 对中文进行分词\n",
    "    :return: 分词后的字符串（每个词用空格隔开）\n",
    "    \"\"\"\n",
    "    # 使用 jieba 对中文文本进行分词，返回的是一个生成器\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\")\n",
    "    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n",
    "    con3 = jieba.cut(\n",
    "        \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n",
    "\n",
    "    # 打印生成器的类型\n",
    "    print(\"生成器类型：\")\n",
    "    print(type(con1))\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 把生成器转换成列表\n",
    "    content1 = list(con1)\n",
    "    content2 = list(con2)\n",
    "    content3 = list(con3)\n",
    "\n",
    "    # 打印分词结果\n",
    "    print(\"分词结果：\")\n",
    "    print(content1)\n",
    "    print(content2)\n",
    "    print(content3)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 把列表转换成字符串，每个词之间用空格隔开\n",
    "    c1 = ' '.join(content1)\n",
    "    c2 = ' '.join(content2)\n",
    "    c3 = ' '.join(content3)\n",
    "\n",
    "    return c1, c2, c3\n",
    "\n",
    "\n",
    "def hanzivec():\n",
    "    \"\"\"\n",
    "    中文特征值化\n",
    "    使用 CountVectorizer 对分词后的中文文本进行特征提取\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 调用 cutword 函数，获取分词后的字符串\n",
    "    c1, c2, c3 = cutword()\n",
    "\n",
    "    # 打印分词后的字符串\n",
    "    print(\"分词后的字符串：\")\n",
    "    print(c1)\n",
    "    print(c2)\n",
    "    print(c3)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 初始化 CountVectorizer 对象\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    # 使用 fit_transform 将分词后的文本转换为词频矩阵\n",
    "    data = cv.fit_transform([c1, c2, c3])\n",
    "\n",
    "    # 打印词汇表（特征名称）\n",
    "    print(\"词汇表（特征名称）：\")\n",
    "    print(cv.get_feature_names_out())\n",
    "\n",
    "    # 将稀疏矩阵转换为稠密矩阵（二维数组），并打印\n",
    "    print(\"词频矩阵：\")\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用 hanzivec 函数，运行示例\n",
    "hanzivec()"
   ],
   "id": "93da8f16b2b0b850",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成器类型：\n",
      "<class 'generator'>\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.340 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果：\n",
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "分词后的字符串：\n",
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。\n",
      "我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。\n",
      "如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "--------------------------------------------------\n",
      "词汇表（特征名称）：\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "词频矩阵：\n",
      "[[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TfidfVectorizer",
   "id": "d7f2c9792351a598"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:02:19.809647Z",
     "start_time": "2025-01-09T10:02:19.798788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tfidf_vectorizer_demo():\n",
    "    \"\"\"\n",
    "    中文特征值化，计算 TF-IDF 值\n",
    "    使用 TfidfVectorizer 对分词后的中文文本进行 TF-IDF 特征提取\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 调用 cutword 函数，获取分词后的字符串\n",
    "    c1, c2, c3 = cutword()\n",
    "\n",
    "    # 打印分词后的字符串\n",
    "    print(\"分词后的字符串：\")\n",
    "    print(c1)\n",
    "    print(c2)\n",
    "    print(c3)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 初始化 TfidfVectorizer 对象，设置 smooth_idf=True 以平滑 IDF 值\n",
    "    tf = TfidfVectorizer(smooth_idf=True)\n",
    "\n",
    "    # 使用 fit_transform 将分词后的文本转换为 TF-IDF 矩阵\n",
    "    data = tf.fit_transform([c1, c2, c3])\n",
    "\n",
    "    # 打印词汇表（特征名称）\n",
    "    print(\"词汇表（特征名称）：\")\n",
    "    print(tf.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 打印 TF-IDF 矩阵的类型（稀疏矩阵）\n",
    "    print(\"TF-IDF 矩阵类型：\")\n",
    "    print(type(data))\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 将稀疏矩阵转换为稠密矩阵（二维数组），并打印\n",
    "    print(\"TF-IDF 矩阵：\")\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用 tfidf_vectorizer_demo 函数，运行示例\n",
    "tfidf_vectorizer_demo()"
   ],
   "id": "54e1bed99d91cc4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成器类型：\n",
      "<class 'generator'>\n",
      "--------------------------------------------------\n",
      "分词结果：\n",
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "分词后的字符串：\n",
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。\n",
      "我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。\n",
      "如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "--------------------------------------------------\n",
      "词汇表（特征名称）：\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "--------------------------------------------------\n",
      "TF-IDF 矩阵类型：\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "TF-IDF 矩阵：\n",
      "[[0.         0.         0.21821789 0.         0.         0.\n",
      "  0.43643578 0.         0.         0.         0.         0.\n",
      "  0.21821789 0.         0.21821789 0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.43643578 0.\n",
      "  0.21821789 0.         0.43643578 0.21821789 0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.2410822  0.         0.\n",
      "  0.         0.2410822  0.2410822  0.2410822  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2410822\n",
      "  0.55004769 0.         0.         0.         0.         0.2410822\n",
      "  0.         0.         0.         0.         0.48216441 0.\n",
      "  0.         0.         0.         0.         0.2410822  0.2410822 ]\n",
      " [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891\n",
      "  0.         0.         0.         0.         0.15698297 0.15698297\n",
      "  0.         0.15698297 0.         0.15698297 0.15698297 0.\n",
      "  0.1193896  0.         0.         0.15698297 0.         0.\n",
      "  0.         0.15698297 0.         0.         0.         0.31396594\n",
      "  0.15698297 0.         0.         0.15698297 0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### homework2：完成MinMaxScaler, StandardScaler的练习，并理解其原理",
   "id": "8405f715799f9c2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MinMaxScaler",
   "id": "4c2cc2cbe60ea6c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![本地图片示例](./images/minmaxscaler.png )",
   "id": "ea6b0dc2ac077745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:10:45.598418Z",
     "start_time": "2025-01-09T10:10:45.592015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mm():\n",
    "    \"\"\"\n",
    "    归一化处理\n",
    "    使用 MinMaxScaler 对数据进行归一化，将特征值缩放到指定范围（默认是 [0, 1]）\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 初始化 MinMaxScaler 对象，设置特征值范围为 [0, 1]\n",
    "    # feature_range 参数用于指定归一化后的范围，默认是 (0, 1)\n",
    "    mm = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # 输入数据，进行拟合和转换\n",
    "    # fit_transform 会计算数据的最小值和最大值，并将数据缩放到指定范围\n",
    "    # fit_transform 一般用于训练集，transform 一般用于测试集\n",
    "    data = mm.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "\n",
    "    # 打印归一化后的数据\n",
    "    print(\"归一化后的数据：\")\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 使用 transform 对新的数据进行归一化\n",
    "    # transform 不会重新计算最小值和最大值，而是使用之前 fit_transform 计算的结果\n",
    "    out = mm.transform([[1, 2, 3, 4], [6, 5, 8, 7]])\n",
    "\n",
    "    # 打印新数据的归一化结果\n",
    "    print(\"新数据的归一化结果：\")\n",
    "    print(out)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用 mm 函数，运行示例\n",
    "mm()"
   ],
   "id": "1b1fe96e5d136045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化后的数据：\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         1.         0.83333333]\n",
      " [0.5        0.5        0.6        1.        ]]\n",
      "--------------------------------------------------\n",
      "新数据的归一化结果：\n",
      "[[-1.96666667  0.         -1.4        -6.        ]\n",
      " [-1.8         1.5        -0.4        -5.5       ]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "StandardScaler",
   "id": "76a54471d65fb69b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![本地图片示例](./images/standardscaler.png )",
   "id": "d453158fd5007846"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:14:09.639257Z",
     "start_time": "2025-01-09T10:14:09.631335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stand():\n",
    "    \"\"\"\n",
    "    标准化缩放\n",
    "    使用 StandardScaler 对数据进行标准化处理，使数据均值为 0，方差为 1\n",
    "    注意：标准化后的数据不是标准正态分布，只是均值为 0，方差为 1 的分布\n",
    "    :return: 标准化后的数据\n",
    "    \"\"\"\n",
    "    # 初始化 StandardScaler 对象\n",
    "    std = StandardScaler()\n",
    "\n",
    "    # 输入数据，进行拟合和转换\n",
    "    # fit_transform 会计算数据的均值和方差，并将数据标准化\n",
    "    data = std.fit_transform([[1., -1., 3.], [2., 4., 2.], [4., 6., -1.]])\n",
    "\n",
    "    # 打印标准化后的数据\n",
    "    print(\"标准化后的数据：\")\n",
    "    print(data)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 打印每列的均值\n",
    "    print(\"每列的均值：\")\n",
    "    print(std.mean_)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 打印每列的方差\n",
    "    print(\"每列的方差：\")\n",
    "    print(std.var_)\n",
    "\n",
    "    # 打印已处理的样本数\n",
    "    print(\"已处理的样本数：\")\n",
    "    print(std.n_samples_seen_)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# 调用 stand 函数，运行示例\n",
    "data = stand()"
   ],
   "id": "2222f4005a24b2b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化后的数据：\n",
      "[[-1.06904497 -1.35873244  0.98058068]\n",
      " [-0.26726124  0.33968311  0.39223227]\n",
      " [ 1.33630621  1.01904933 -1.37281295]]\n",
      "--------------------------------------------------\n",
      "每列的均值：\n",
      "[2.33333333 3.         1.33333333]\n",
      "--------------------------------------------------\n",
      "每列的方差：\n",
      "[1.55555556 8.66666667 2.88888889]\n",
      "已处理的样本数：\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### homework3：完成SimpleImputer的练习",
   "id": "9b1398f6eb452b96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:16:48.164016Z",
     "start_time": "2025-01-09T10:16:48.157597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def im():\n",
    "    \"\"\"\n",
    "    缺失值处理\n",
    "    使用 SimpleImputer 对缺失值进行填补\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 初始化 SimpleImputer 对象\n",
    "    # missing_values=np.nan 表示缺失值的类型是 NaN\n",
    "    # strategy='mean' 表示使用均值填补缺失值\n",
    "    im = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # 输入数据，包含缺失值（NaN）\n",
    "    # fit_transform 会计算每列的均值，并用均值填补缺失值\n",
    "    data = im.fit_transform([[1, 2], [np.nan, 3], [7, 6], [3, 2]])\n",
    "\n",
    "    # 打印填补后的数据\n",
    "    print(\"填补后的数据：\")\n",
    "    print(data)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用 im 函数，运行示例\n",
    "im()"
   ],
   "id": "c4f40569632321af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "填补后的数据：\n",
      "[[1.         2.        ]\n",
      " [3.66666667 3.        ]\n",
      " [7.         6.        ]\n",
      " [3.         2.        ]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### homework4：完成VarianceThreshold，PCA的练习并理解原理",
   "id": "8ae9794f1c9ff167"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "VarianceThreshold",
   "id": "38002a609bf36269"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:18:00.520704Z",
     "start_time": "2025-01-09T10:18:00.516603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def var():\n",
    "    \"\"\"\n",
    "    特征选择 - 删除低方差的特征\n",
    "    使用 VarianceThreshold 删除方差低于指定阈值的特征\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 初始化 VarianceThreshold 对象\n",
    "    # threshold=0.1 表示删除方差小于 0.1 的特征\n",
    "    # 默认情况下，threshold=0，即只删除方差为 0 的特征\n",
    "    var = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "    # 输入数据\n",
    "    # fit_transform 会计算每列的方差，并删除方差小于阈值的特征\n",
    "    data = var.fit_transform([[0, 2, 0, 3],\n",
    "                              [0, 1, 4, 3],\n",
    "                              [0, 1, 1, 3]])\n",
    "\n",
    "    # 打印处理后的数据\n",
    "    print(\"处理后的数据：\")\n",
    "    print(data)\n",
    "\n",
    "    # 获取剩余特征的列编号\n",
    "    print('剩余特征的列编号：%s' % var.get_support(True))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用 var 函数，运行示例\n",
    "var()"
   ],
   "id": "d046909f1360b148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据：\n",
      "[[2 0]\n",
      " [1 4]\n",
      " [1 1]]\n",
      "剩余特征的列编号：[1 2]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PCA",
   "id": "ce15ea10d7561c5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:22:21.322369Z",
     "start_time": "2025-01-09T10:22:21.313825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pca():\n",
    "    \"\"\"\n",
    "    主成分分析（PCA）进行特征降维\n",
    "    使用 PCA 对数据进行降维，保留指定比例的方差\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 原始数据\n",
    "    original_value = np.array([[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]])\n",
    "\n",
    "    # 计算原始数据的总方差\n",
    "    print(\"原始数据的总方差：\")\n",
    "    print(np.var(original_value, axis=0).sum())\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 初始化 PCA 对象\n",
    "    # n_components=0.9 表示保留 90% 的方差\n",
    "    # n_components 可以是小数（表示保留的方差比例）或整数（表示降维后的特征数量）\n",
    "    pca = PCA(n_components=0.9)\n",
    "\n",
    "    # 对原始数据进行拟合和转换\n",
    "    # fit_transform 会计算主成分，并将数据转换到新的低维空间\n",
    "    data = pca.fit_transform(original_value)\n",
    "\n",
    "    # 打印降维后的数据\n",
    "    print(\"降维后的数据：\")\n",
    "    print(data)\n",
    "    print(\"数据类型：\")\n",
    "    print(type(data))\n",
    "\n",
    "    # 计算降维后数据的总方差\n",
    "    print(\"降维后数据的总方差：\")\n",
    "    print(np.var(data, axis=0).sum())\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 打印每个主成分解释的方差比例\n",
    "    print(\"每个主成分解释的方差比例：\")\n",
    "    print(pca.explained_variance_ratio_)\n",
    "\n",
    "    # 打印所有主成分解释的方差比例之和\n",
    "    print(\"所有主成分解释的方差比例之和：\")\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# 调用 pca 函数，运行示例\n",
    "pca()"
   ],
   "id": "a906b3b849b5623c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据的总方差：\n",
      "29.333333333333336\n",
      "--------------------------------------------------\n",
      "降维后的数据：\n",
      "[[-1.28620952e-15  3.82970843e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]]\n",
      "数据类型：\n",
      "<class 'numpy.ndarray'>\n",
      "降维后数据的总方差：\n",
      "29.333333333333332\n",
      "--------------------------------------------------\n",
      "每个主成分解释的方差比例：\n",
      "[0.75 0.25]\n",
      "所有主成分解释的方差比例之和：\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### homework5：完成load及fetch数据加载，搞清数据集的特征和目标，train_test_split样本切分",
   "id": "dbc78d63d40ddde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "load数据加载",
   "id": "32764badfb1bac97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:16:44.180090Z",
     "start_time": "2025-01-09T14:16:44.176067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from pprint import pprint"
   ],
   "id": "e00ff307dc1bdb33",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:26:22.920832Z",
     "start_time": "2025-01-09T10:26:22.908381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载鸢尾花数据集\n",
    "li = load_iris()\n",
    "\n",
    "# 打印特征值\n",
    "print(\"获取特征值：\")\n",
    "# 查看特征值的数据类型\n",
    "print(type(li.data))  # 输出特征值的数据类型\n",
    "print('-' * 50)\n",
    "\n",
    "# 查看特征值的形状（样本量和特征数量）\n",
    "# shape 返回一个元组，格式为 (样本数量, 特征数量)\n",
    "print(\"特征值的形状（样本量, 特征数量）：\")\n",
    "print(li.data.shape)  # 输出 (150, 4)，表示 150 个样本，每个样本有 4 个特征\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印特征值（实际数据）\n",
    "print(\"特征值数据：\")\n",
    "print(li.data)  # 输出所有样本的特征值"
   ],
   "id": "d599be72d61cf3bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值：\n",
      "<class 'numpy.ndarray'>\n",
      "--------------------------------------------------\n",
      "特征值的形状（样本量, 特征数量）：\n",
      "(150, 4)\n",
      "--------------------------------------------------\n",
      "特征值数据：\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:27:19.855596Z",
     "start_time": "2025-01-09T10:27:19.849708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印目标值\n",
    "print(\"目标值：\")\n",
    "# 目标值是每个样本对应的类别标签，是一个一维数组\n",
    "print(li.target)  # 输出目标值\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印数据集的描述信息\n",
    "print(\"数据集描述信息：\")\n",
    "# DESCR 是数据集的详细描述，包括数据集的基本信息、特征描述等\n",
    "print(li.DESCR)  # 输出数据集的描述信息\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印特征名称\n",
    "print(\"特征名称：\")\n",
    "# feature_names 是每个特征的名字，是一个列表\n",
    "print(li.feature_names)  # 输出特征名称\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印目标类别名称\n",
    "print(\"目标类别名称：\")\n",
    "# target_names 是每个目标类别的名字，是一个列表\n",
    "print(li.target_names)  # 输出目标类别名称"
   ],
   "id": "59187a285b74263e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标值：\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "--------------------------------------------------\n",
      "数据集描述信息：\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n",
      "--------------------------------------------------\n",
      "特征名称：\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "--------------------------------------------------\n",
      "目标类别名称：\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:27:43.764689Z",
     "start_time": "2025-01-09T10:27:43.759902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印特征值的形状\n",
    "print(\"特征值的形状：\")\n",
    "# li.data.shape 返回一个元组，格式为 (样本数量, 特征数量)\n",
    "# 对于鸢尾花数据集，输出为 (150, 4)，表示 150 个样本，每个样本有 4 个特征\n",
    "print(li.data.shape)  # 输出 (150, 4)\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印目标值的形状\n",
    "print(\"目标值的形状：\")\n",
    "# li.target.shape 返回一个元组，表示目标值的形状\n",
    "# 对于鸢尾花数据集，输出为 (150,)，表示 150 个样本的目标值\n",
    "print(li.target.shape)  # 输出 (150,)"
   ],
   "id": "2d2021759fab92e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值的形状：\n",
      "(150, 4)\n",
      "--------------------------------------------------\n",
      "目标值的形状：\n",
      "(150,)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:28:26.378206Z",
     "start_time": "2025-01-09T10:28:26.366242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 划分训练集和测试集\n",
    "# train_test_split 用于将数据集划分为训练集和测试集\n",
    "# li.data 是特征值，li.target 是目标值\n",
    "# test_size=0.25 表示测试集占总数据的 25%，训练集占 75%\n",
    "# random_state=1 设置随机种子，确保每次运行代码时划分结果一致\n",
    "x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25, random_state=1)\n",
    "\n",
    "# 打印训练集的特征值和目标值\n",
    "print(\"训练集特征值和目标值：\")\n",
    "print(\"特征值：\", x_train)  # 训练集特征值\n",
    "print(\"目标值：\", y_train)  # 训练集目标值\n",
    "print(\"训练集特征值 shape：\", x_train.shape)  # 训练集特征值的形状\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印测试集的特征值和目标值\n",
    "print(\"测试集特征值和目标值：\")\n",
    "print(\"特征值：\", x_test)  # 测试集特征值\n",
    "print(\"目标值：\", y_test)  # 测试集目标值\n",
    "print(\"测试集特征值 shape：\", x_test.shape)  # 测试集特征值的形状"
   ],
   "id": "244a977fd6cce865",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征值和目标值：\n",
      "特征值： [[6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.6 1.4 0.1]]\n",
      "目标值： [1 2 2 0 2 2 1 2 0 0 0 1 0 0 2 2 2 2 2 1 2 1 0 2 2 0 0 2 0 2 2 1 1 2 2 0 1\n",
      " 1 2 1 2 1 0 0 0 2 0 1 2 2 0 0 1 0 2 1 2 2 1 2 2 1 0 1 0 1 1 0 1 0 0 2 2 2\n",
      " 0 0 1 0 2 0 2 2 0 2 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 2 0 0 2 1 2 1 2 2 1 2\n",
      " 0]\n",
      "训练集特征值 shape： (112, 4)\n",
      "--------------------------------------------------\n",
      "测试集特征值和目标值：\n",
      "特征值： [[5.8 4.  1.2 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 3.4 1.4 0.2]]\n",
      "目标值： [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0]\n",
      "测试集特征值 shape： (38, 4)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "fetch数据加载",
   "id": "f63f0733ea2133d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:11:23.235058Z",
     "start_time": "2025-01-09T14:11:23.063620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载 20 类新闻数据集\n",
    "# subset='all' 表示下载全部数据（包括训练集和测试集）\n",
    "# data_home='../data' 表示将数据存储到当前目录的上一级目录中的 data 文件夹\n",
    "news = fetch_20newsgroups(subset='all', data_home='../data')\n",
    "\n",
    "# 打印第一个样本的内容\n",
    "print('第一个样本：')\n",
    "print(news.data[0])  # 输出第一个样本的文本内容\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印特征类型\n",
    "print('特征类型：')\n",
    "print(type(news.data))  # 输出特征的数据类型，通常是列表\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印前 15 个样本的目标值（类别标签）\n",
    "print('前 15 个样本的目标值：')\n",
    "print(news.target[0:15])  # 输出前 15 个样本的类别标签\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印目标类别名称\n",
    "# 使用 pprint 格式化输出，使列表更易读\n",
    "print('目标类别名称：')\n",
    "pprint(list(news.target_names))  # 输出所有目标类别的名称"
   ],
   "id": "803f0b5a2847b087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个样本：\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "特征类型：\n",
      "<class 'list'>\n",
      "--------------------------------------------------\n",
      "前 15 个样本的目标值：\n",
      "[10  3 17  3  4 12  4 10 10 19 19 11 19 13  0]\n",
      "--------------------------------------------------\n",
      "目标类别名称：\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:14:13.742469Z",
     "start_time": "2025-01-09T14:14:13.734511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印分隔线\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印数据集中的样本数量\n",
    "# len(news.data) 返回数据集中的样本数量\n",
    "print(len(news.data))  # 输出数据集中的样本数量\n",
    "\n",
    "# 打印提示信息\n",
    "print('新闻所有的标签')\n",
    "\n",
    "# 打印数据集的目标值（标签）\n",
    "# news.target 是一个一维数组，表示每个样本的类别标签\n",
    "print(news.target)  # 输出所有样本的类别标签\n",
    "\n",
    "# 打印分隔线\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印目标值的最小值和最大值\n",
    "# min(news.target) 返回目标值的最小值\n",
    "# max(news.target) 返回目标值的最大值\n",
    "print(min(news.target), max(news.target))  # 输出目标值的最小值和最大值"
   ],
   "id": "1d65d203fb31a1e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "18846\n",
      "新闻所有的标签\n",
      "[10  3 17 ...  3  1  7]\n",
      "--------------------------------------------------\n",
      "0 19\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:17:52.484515Z",
     "start_time": "2025-01-09T14:17:52.469978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载加州房价数据集\n",
    "# data_home='data' 表示将数据存储到当前目录下的 data 文件夹中\n",
    "house = fetch_california_housing(data_home='../data')\n",
    "\n",
    "# 打印提示信息\n",
    "print(\"获取特征值\")\n",
    "\n",
    "# 打印第一个样本的特征值\n",
    "# house.data[0] 返回第一个样本的特征值，是一个一维数组\n",
    "print(house.data[0])  # 输出第一个样本的特征值\n",
    "\n",
    "# 打印样本的形状\n",
    "# house.data.shape 返回数据集的形状，格式为 (样本数量, 特征数量)\n",
    "print('样本的形状')\n",
    "print(house.data.shape)  # 输出数据集的形状\n",
    "\n",
    "# 打印分隔线\n",
    "print('-' * 50)"
   ],
   "id": "2efcbe77556849c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "[   8.3252       41.            6.98412698    1.02380952  322.\n",
      "    2.55555556   37.88       -122.23      ]\n",
      "样本的形状\n",
      "(20640, 8)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:18:32.720263Z",
     "start_time": "2025-01-09T14:18:32.715698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印提示信息\n",
    "print(\"目标值\")\n",
    "\n",
    "# 打印前 10 个样本的目标值\n",
    "# house.target[0:10] 返回前 10 个样本的目标值，是一个一维数组\n",
    "print(house.target[0:10])  # 输出前 10 个样本的目标值\n",
    "\n",
    "# 打印分隔线\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印数据集的描述信息\n",
    "# house.DESCR 返回数据集的详细描述，包括数据集的基本信息、特征描述等\n",
    "print(house.DESCR)  # 输出数据集的描述信息\n",
    "\n",
    "# 打印分隔线\n",
    "print('-' * 50)\n",
    "\n",
    "# 打印特征名称\n",
    "# house.feature_names 返回每个特征的名字，是一个列表\n",
    "print(house.feature_names)  # 输出特征名称\n",
    "\n",
    "# 打印分隔线\n",
    "print('-' * 50)"
   ],
   "id": "7e61b30f882148fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标值\n",
      "[4.526 3.585 3.521 3.413 3.422 2.697 2.992 2.414 2.267 2.611]\n",
      "--------------------------------------------------\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "--------------------------------------------------\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
