{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### homework1：练习Seq2Seq数据预处理部分",
   "id": "7111b6676612bfe"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-25T10:47:38.616515Z",
     "start_time": "2025-01-25T10:47:33.490701Z"
    }
   },
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.10.0\n",
      "numpy 1.26.4\n",
      "pandas 2.2.3\n",
      "sklearn 1.6.0\n",
      "torch 2.5.1+cu124\n",
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "数据加载",
   "id": "b1829526241159ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:02:41.009301Z",
     "start_time": "2025-01-25T11:02:40.893966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 定义一个函数，将Unicode字符串转换为ASCII字符串\n",
    "def unicode_to_ascii(s):\n",
    "    # 使用unicodedata.normalize方法将字符串s转换为NFD形式。\n",
    "    # NFD（Normalization Form D）是一种Unicode规范化形式，它将字符分解为基本字符和组合字符。\n",
    "    # 例如，字符'é'会被分解为'e'和一个组合的重音符号。\n",
    "    normalized_string = unicodedata.normalize('NFD', s)\n",
    "    \n",
    "    # 使用列表推导式遍历规范化后的字符串中的每个字符。\n",
    "    # unicodedata.category(c)返回字符c的Unicode类别。\n",
    "    # 'Mn'表示“非间距标记”（例如重音符号），我们通过条件判断去除这些字符。\n",
    "    ascii_string = ''.join(c for c in normalized_string if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # 返回转换后的ASCII字符串\n",
    "    return ascii_string\n",
    "\n",
    "# 测试unicode_to_ascii函数\n",
    "# 定义一个英文句子和一个西班牙语句子，使用u前缀表示Unicode字符串\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "\n",
    "# 打印转换后的英文句子\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "# 输出: May I borrow this book?\n",
    "\n",
    "# 打印转换后的西班牙语句子\n",
    "print(unicode_to_ascii(sp_sentence))\n",
    "# 输出: ¿Puedo tomar prestado este libro?"
   ],
   "id": "6aed330ad8b626ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May I borrow this book?\n",
      "¿Puedo tomar prestado este libro?\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:02:44.353138Z",
     "start_time": "2025-01-25T11:02:44.345976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_sentence(w):\n",
    "    # 1. 将字符串转换为小写，并去掉首尾多余的空格\n",
    "    # 2. 调用 unicode_to_ascii 函数，将 Unicode 字符转换为 ASCII 字符（去除重音符号等）\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # 例如：\"he is a boy.\" => \"he is a boy . \"\n",
    "    # 使用正则表达式匹配标点符号（?.!,¿），并在其前后添加空格\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "\n",
    "    # 将多个连续的空格替换为一个空格\n",
    "    # 因为上一步可能会引入多余的空格\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # 除了字母 (a-z, A-Z) 和标点符号（\".\", \"?\", \"!\", \",\", \"¿\"），将所有其他字符替换为空格\n",
    "    # 这样可以去除数字、特殊符号等不需要的字符\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    # 去掉字符串首尾的空格，确保最终结果干净\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    return w\n",
    "\n",
    "# 测试 preprocess_sentence 函数\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "\n",
    "# 打印处理后的英文句子\n",
    "print(preprocess_sentence(en_sentence))\n",
    "# 输出: may i borrow this book ?\n",
    "\n",
    "# 打印处理后的西班牙语句子\n",
    "print(preprocess_sentence(sp_sentence))\n",
    "# 输出: ¿ puedo tomar prestado este libro ?\n",
    "\n",
    "# 打印处理后的西班牙语句子的 UTF-8 编码\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))\n",
    "# 输出: b' \\xc2\\xbf puedo tomar prestado este libro ?'"
   ],
   "id": "72d96f018220215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may i borrow this book ?\n",
      "¿ puedo tomar prestado este libro ?\n",
      "b'\\xc2\\xbf puedo tomar prestado este libro ?'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset",
   "id": "a14c0085f773449d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:25:03.468504Z",
     "start_time": "2025-01-25T11:25:03.462313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#zip例子\n",
    "a = [[1,2],[4,5],[7,8]]\n",
    "zipped = list(zip(*a))\n",
    "print(zipped)"
   ],
   "id": "cd2fadc5f18707ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4, 7), (2, 5, 8)]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:25:18.145651Z",
     "start_time": "2025-01-25T11:25:18.136212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_index1 = np.random.choice(a=[\"train\", \"test\"], replace=True, p=[0.9, 0.1], size=100)\n",
    "split_index1"
   ],
   "id": "1e118a364a64a2b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'test', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'test', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'test', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'test', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'test', 'train', 'test', 'train', 'train', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train'], dtype='<U5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:28:02.888231Z",
     "start_time": "2025-01-25T11:28:02.415317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class LangPairDataset(Dataset):\n",
    "    # 数据文件路径\n",
    "    fpath = Path(r\"./data_spa_en/spa.txt\")\n",
    "    # 缓存文件路径\n",
    "    cache_path = Path(r\"./.cache/lang_pair.npy\")\n",
    "    # 按照 9:1 的比例随机划分训练集和测试集\n",
    "    split_index = np.random.choice(a=[\"train\", \"test\"], replace=True, p=[0.9, 0.1], size=118964)\n",
    "\n",
    "    def __init__(self, mode=\"train\", cache=False):\n",
    "        # 如果没有缓存，或者缓存文件不存在，则处理数据并保存缓存\n",
    "        if cache or not self.cache_path.exists():\n",
    "            # 创建缓存文件夹（如果不存在）\n",
    "            self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            # 打开数据文件并读取所有行\n",
    "            with open(self.fpath, \"r\", encoding=\"utf8\") as file:\n",
    "                lines = file.readlines()\n",
    "                # 对每一行数据进行预处理，分割成目标语言和源语言\n",
    "                lang_pair = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines]\n",
    "                # 将目标语言和源语言分离\n",
    "                trg, src = zip(*lang_pair)\n",
    "                # 转换为 numpy 数组\n",
    "                trg = np.array(trg)\n",
    "                src = np.array(src)\n",
    "                # 将数据保存为 npy 文件，方便下次直接读取\n",
    "                np.save(self.cache_path, {\"trg\": trg, \"src\": src})\n",
    "        else:\n",
    "            # 如果缓存文件存在，则直接加载缓存数据\n",
    "            lang_pair = np.load(self.cache_path, allow_pickle=True).item()\n",
    "            trg = lang_pair[\"trg\"]\n",
    "            src = lang_pair[\"src\"]\n",
    "\n",
    "        # 根据 mode（train/test）从 split_index 中筛选出对应的数据\n",
    "        self.trg = trg[self.split_index == mode]  # 目标语言（英语）\n",
    "        self.src = src[self.split_index == mode]  # 源语言（西班牙语）\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 返回指定索引的源语言和目标语言对\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的长度\n",
    "        return len(self.src)\n",
    "\n",
    "\n",
    "# 创建训练集和测试集实例\n",
    "train_ds = LangPairDataset(\"train\")\n",
    "test_ds = LangPairDataset(\"test\")"
   ],
   "id": "962cba622942e8d9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:28:43.987633Z",
     "start_time": "2025-01-25T11:28:43.983643Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"source: {}\\ntarget: {}\".format(*train_ds[-1]))",
   "id": "917e75e45fb263d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado .\n",
      "target: if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo .\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tokenizer",
   "id": "3bbbde103a3d9f81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这里有两种处理方式，分别对应着 encoder 和 decoder 的 word embedding 是否共享，这里实现不共享的方案。",
   "id": "22e7ca747a029ec3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:41:21.926485Z",
     "start_time": "2025-01-25T11:41:21.552251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_word_idx(ds, mode=\"src\", threshold=2):\n",
    "    # 初始化词表，包含特殊 token\n",
    "    word2idx = {\n",
    "        \"[PAD]\": 0,     # 填充 token，用于填充短句子\n",
    "        \"[BOS]\": 1,     # 句子开始 token\n",
    "        \"[UNK]\": 2,     # 未知 token，用于表示词表中未出现的词\n",
    "        \"[EOS]\": 3,     # 句子结束 token\n",
    "    }\n",
    "    # 反向词表，用于根据索引查找单词\n",
    "    idx2word = {value: key for key, value in word2idx.items()}\n",
    "    # 当前词表的下一个可用索引\n",
    "    index = len(idx2word)\n",
    "    # 设置词频阈值，低于此阈值的单词将被忽略\n",
    "    threshold = 1\n",
    "\n",
    "    # 将数据集中所有句子拼接成一个长字符串，然后按空格分割成单词列表\n",
    "    # 如果数据集很大，建议使用 for 循环逐句处理，避免内存不足\n",
    "    word_list = \" \".join([pair[0 if mode == \"src\" else 1] for pair in ds]).split()\n",
    "    # 统计单词频率\n",
    "    counter = Counter(word_list)\n",
    "    print(\"word count:\", len(counter))\n",
    "\n",
    "    # 遍历单词频率统计结果\n",
    "    for token, count in counter.items():\n",
    "        # 如果单词出现次数大于等于阈值，则加入词表\n",
    "        if count >= threshold:\n",
    "            word2idx[token] = index  # 加入正向词表\n",
    "            idx2word[index] = token  # 加入反向词表\n",
    "            index += 1  # 更新下一个可用索引\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "# 生成源语言（西班牙语）和目标语言（英语）的词表\n",
    "src_word2idx, src_idx2word = get_word_idx(train_ds, \"src\")  # 源语言词表（西班牙语）\n",
    "trg_word2idx, trg_idx2word = get_word_idx(train_ds, \"trg\")  # 目标语言词表（英语）"
   ],
   "id": "f5593f33d79d0118",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count: 23745\n",
      "word count: 12475\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T11:41:31.094916Z",
     "start_time": "2025-01-25T11:41:31.079862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, word2idx, idx2word, max_length=500, pad_idx=0, bos_idx=1, eos_idx=3, unk_idx=2):\n",
    "        # 初始化词表和参数\n",
    "        self.word2idx = word2idx  # 单词到索引的映射\n",
    "        self.idx2word = idx2word  # 索引到单词的映射\n",
    "        self.max_length = max_length  # 最大序列长度\n",
    "        self.pad_idx = pad_idx  # 填充 token 的索引\n",
    "        self.bos_idx = bos_idx  # 句子开始 token 的索引\n",
    "        self.eos_idx = eos_idx  # 句子结束 token 的索引\n",
    "        self.unk_idx = unk_idx  # 未知 token 的索引\n",
    "\n",
    "    def encode(self, text_list, padding_first=False, add_bos=True, add_eos=True, return_mask=False):\n",
    "        \"\"\"将文本列表编码为索引列表\n",
    "        - padding_first: 是否在前面填充\n",
    "        - add_bos: 是否添加句子开始 token\n",
    "        - add_eos: 是否添加句子结束 token\n",
    "        - return_mask: 是否返回掩码（用于指示哪些是填充 token）\n",
    "        \"\"\"\n",
    "        # 计算最大长度（不超过 self.max_length）\n",
    "        max_length = min(self.max_length, add_eos + add_bos + max([len(text) for text in text_list]))\n",
    "        indices_list = []  # 存储编码后的索引列表\n",
    "\n",
    "        for text in text_list:\n",
    "            # 将单词转换为索引，未知单词用 unk_idx 代替\n",
    "            indices = [self.word2idx.get(word, self.unk_idx) for word in text[:max_length - add_bos - add_eos]]\n",
    "            # 添加句子开始 token\n",
    "            if add_bos:\n",
    "                indices = [self.bos_idx] + indices\n",
    "            # 添加句子结束 token\n",
    "            if add_eos:\n",
    "                indices = indices + [self.eos_idx]\n",
    "            # 在前面或后面填充\n",
    "            if padding_first:\n",
    "                indices = [self.pad_idx] * (max_length - len(indices)) + indices\n",
    "            else:\n",
    "                indices = indices + [self.pad_idx] * (max_length - len(indices))\n",
    "            indices_list.append(indices)\n",
    "\n",
    "        # 将索引列表转换为 tensor\n",
    "        input_ids = torch.tensor(indices_list)\n",
    "        # 生成掩码（1 表示填充 token，0 表示真实 token）\n",
    "        masks = (input_ids == self.pad_idx).to(dtype=torch.int64)\n",
    "\n",
    "        # 返回编码结果和掩码（如果 return_mask 为 True）\n",
    "        return input_ids if not return_mask else (input_ids, masks)\n",
    "\n",
    "    def decode(self, indices_list, remove_bos=True, remove_eos=True, remove_pad=True, split=False):\n",
    "        \"\"\"将索引列表解码为文本列表\n",
    "        - remove_bos: 是否移除句子开始 token\n",
    "        - remove_eos: 是否移除句子结束 token\n",
    "        - remove_pad: 是否移除填充 token\n",
    "        - split: 是否返回单词列表（而不是拼接的句子）\n",
    "        \"\"\"\n",
    "        text_list = []\n",
    "        for indices in indices_list:\n",
    "            text = []\n",
    "            for index in indices:\n",
    "                word = self.idx2word.get(index, \"[UNK]\")  # 将索引转换为单词，未知索引用 \"[UNK]\" 代替\n",
    "                # 根据参数决定是否移除特殊 token\n",
    "                if remove_bos and word == \"[BOS]\":\n",
    "                    continue\n",
    "                if remove_eos and word == \"[EOS]\":\n",
    "                    break\n",
    "                if remove_pad and word == \"[PAD]\":\n",
    "                    break\n",
    "                text.append(word)\n",
    "            # 将单词列表拼接为句子或直接返回单词列表\n",
    "            text_list.append(\" \".join(text) if not split else text)\n",
    "        return text_list\n",
    "\n",
    "\n",
    "# 创建源语言和目标语言的 tokenizer\n",
    "src_tokenizer = Tokenizer(word2idx=src_word2idx, idx2word=src_idx2word)  # 源语言 tokenizer\n",
    "trg_tokenizer = Tokenizer(word2idx=trg_word2idx, idx2word=trg_idx2word)  # 目标语言 tokenizer\n",
    "\n",
    "# 测试编码和解码功能\n",
    "raw_text = [\"hello world\".split(), \"tokenize text datas with batch\".split(), \"this is a test\".split()]\n",
    "indices, mask = trg_tokenizer.encode(raw_text, padding_first=False, add_bos=True, add_eos=True, return_mask=True)\n",
    "decode_text = trg_tokenizer.decode(indices.tolist(), remove_bos=False, remove_eos=False, remove_pad=False)\n",
    "\n",
    "# 打印结果\n",
    "print(\"raw text\" + '-' * 10)\n",
    "for raw in raw_text:\n",
    "    print(raw)\n",
    "\n",
    "print(\"mask\" + '-' * 10)\n",
    "for m in mask:\n",
    "    print(m)\n",
    "\n",
    "print(\"indices\" + '-' * 10)\n",
    "for index in indices:\n",
    "    print(index)\n",
    "\n",
    "print(\"decode text\" + '-' * 10)\n",
    "for decode in decode_text:\n",
    "    print(decode)"
   ],
   "id": "ba77ceb4308b87af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text----------\n",
      "['hello', 'world']\n",
      "['tokenize', 'text', 'datas', 'with', 'batch']\n",
      "['this', 'is', 'a', 'test']\n",
      "mask----------\n",
      "tensor([0, 0, 0, 0, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1])\n",
      "indices----------\n",
      "tensor([   1,   17, 3222,    3,    0,    0,    0])\n",
      "tensor([   1,    2, 3883,    2,  737,    2,    3])\n",
      "tensor([   1,  119,  228,  106, 1274,    3,    0])\n",
      "decode text----------\n",
      "[BOS] hello world [EOS] [PAD] [PAD] [PAD]\n",
      "[BOS] [UNK] text [UNK] with [UNK] [EOS]\n",
      "[BOS] this is a test [EOS] [PAD]\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
